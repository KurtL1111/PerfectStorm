import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

class ConvictionScoreCalculator:
    def __init__(self, weights=None):
        """
        Initializes the ConvictionScoreCalculator.

        Args:
            weights (dict, optional): A dictionary to override default weights.
                                      Now includes weights for new ML factors.
                                      Example: {
                                          'adaptive_signals': 0.35, # e.g., RSI, Stoch crossings
                                          'market_regime': 0.25,    # Trend direction/strength
                                          'ml_pattern': 0.15,     # Confidence from Pattern Recognition
                                          'anomaly_score': -0.15, # Acts as a penalty/risk flag
                                          'cluster_modifier': 0.10,# Score based on current market cluster type
                                          'sentiment_score': 0.0 # Placeholder for news/social sentiment
                                          # 'volume_confirmation': 0.05, # Placeholder
                                          # 'volatility_measure': -0.05 # Placeholder, high vol penalty?
                                      }
        """
        default_weights = {
            'adaptive_signals': 0.35,
            'market_regime': 0.25,
            'ml_pattern': 0.15,
            'anomaly_score': -0.10, # Negative weight means high anomaly REDUCES conviction
            'cluster_modifier': 0.10, # Weight for score derived from cluster analysis
            'sentiment_score': 0.05, # Placeholder
        }
        # Merge default and provided weights, allows overriding
        self.weights = {**default_weights, **(weights or {})}
        print(f"ConvictionScoreCalculator initialized with weights: {self.weights}")

    def _normalize_adaptive_signals(self, df):
        """
        Normalizes the contribution of adaptive indicator signals to a -1 to +1 score.
        Assumes df contains 'individual_buy_signals_adaptive' and 'individual_sell_signals_adaptive'.
        These columns should now be calculated based on dynamic thresholds where applicable.
        """
        # Check if the pre-calculated summary columns exist
        buy_col = 'individual_buy_signals_adaptive'
        sell_col = 'individual_sell_signals_adaptive'

        if buy_col not in df.columns or sell_col not in df.columns:
            print(f"Warning: '{buy_col}' or '{sell_col}' not found. Checking for adaptive_* cols...")
            # Fallback: Calculate dynamically if individual adaptive signals exist
            adaptive_buy_cols = [col for col in df.columns if col.startswith('adaptive_buy_')]
            adaptive_sell_cols = [col for col in df.columns if col.startswith('adaptive_sell_')]

            if not adaptive_buy_cols and not adaptive_sell_cols:
                print("Warning: No adaptive signal columns found. Returning neutral signal score (0).")
                return pd.Series(0, index=df.index, dtype=float)

            df[buy_col] = df[adaptive_buy_cols].sum(axis=1) if adaptive_buy_cols else 0
            df[sell_col] = df[adaptive_sell_cols].sum(axis=1) if adaptive_sell_cols else 0
            print("Calculated adaptive summary signals on the fly.")


        buy_signals = df[buy_col]
        sell_signals = df[sell_col]

        # Avoid division by zero and normalize
        total_signals = buy_signals + sell_signals
        score = np.where(total_signals > 0, (buy_signals - sell_signals) / total_signals, 0)

        # Fill potential NaNs introduced during calculation with 0
        return pd.Series(score, index=df.index).fillna(0)


    def _normalize_market_regime(self, df):
        """
        Normalizes market regime score to -1 to +1.
        Uses the 'market_regime' (string) column generated by MarketRegimeDetection.
        """
        col = 'market_regime'
        if col not in df.columns:
            print(f"Warning: '{col}' column not found. Returning neutral regime score (0).")
            return pd.Series(0, index=df.index, dtype=float)

        # More granular regime scores can be added here
        regime_scores = {
            'trending_up': 0.75,
            'strong_uptrend': 0.9,
            'uptrend': 0.7,
            'trending_down': -0.75,
            'strong_downtrend': -0.9,
            'downtrend': -0.7,
            'ranging': 0.0,
            'sideways': 0.0,
            'volatile': -0.25, # Penalize volatility unless other signals confirm direction
            'low_volatility': 0.1, # Slight positive for stability
            'unknown': 0.0,
            # Add mappings for numeric regimes if used
             0: 0.75, # Example mapping
             1: -0.75,
             2: 0.0,
             3: -0.25
        }
        # Apply mapping, defaulting unknown regimes to 0
        score = df[col].apply(lambda x: regime_scores.get(x, 0.0))
        return score.fillna(0) # Fill any NaNs resulting from the apply

    def _normalize_ml_pattern(self, df):
        """
        Uses the 'ml_pattern_confidence' column (-1 to +1) from pattern recognition.
        """
        col = 'ml_pattern_confidence'
        if col not in df.columns:
            #print("Info: 'ml_pattern_confidence' column not found. Returning neutral pattern score (0).")
            return pd.Series(0, index=df.index, dtype=float)
        # Expects confidence score between -1 (strong sell pattern) and +1 (strong buy pattern)
        # Fill NaNs with neutral 0
        return df[col].fillna(0)

    def _normalize_anomaly_score(self, df):
        """
        Uses the 'anomaly_score' column. Assumes higher score means more anomalous.
        We need to decide if anomalies are bullish/bearish or just risk flags.
        Here, we assume it's a risk flag (0=normal, 1=very anomalous), and use a negative weight.
        """
        col = 'anomaly_score'
        if col not in df.columns:
            #print("Info: 'anomaly_score' column not found. Returning neutral anomaly score (0).")
            return pd.Series(0, index=df.index, dtype=float)

        # Normalize the raw score. If it's unbounded, scale it, e.g., to 0-1 or -1 to 1.
        # Example: Using MinMax scaling to 0-1 range where 1 is most anomalous
        # Use a rolling window to avoid lookahead bias if scaling based on series stats
        # For simplicity now, let's clip and potentially scale assuming higher = more anomalous
        # A high score will contribute negatively due to the negative weight
        scaler = MinMaxScaler()
        # Handle potential NaNs and reshape for scaler
        valid_scores = df[col].dropna()
        if valid_scores.empty:
             return pd.Series(0, index=df.index, dtype=float)
        scaled_scores_fit = scaler.fit_transform(valid_scores.values.reshape(-1, 1)).flatten()
        scaled_scores = pd.Series(index=valid_scores.index, data=scaled_scores_fit)

        # Reindex to original DataFrame index, filling missing values with 0 (non-anomalous)
        final_scores = scaled_scores.reindex(df.index).fillna(0)
        return final_scores

    def _normalize_cluster_modifier(self, df):
        """
        Assigns a score based on the current market cluster ID.
        Requires defining the 'meaning' of each cluster.
        Assumes 'ml_cluster_cluster' column exists.
        """
        col = 'ml_cluster_cluster'
        if col not in df.columns:
             #print("Info: 'ml_cluster_cluster' not found. Returning neutral cluster score (0).")
            return pd.Series(0, index=df.index, dtype=float)

        # --- !!! Placeholder: Define Cluster Meanings !!! ---
        # Analyze clustering output offline to determine characteristics. Example:
        cluster_scores = {
            0: 0.5,   # e.g., 'Pre-breakout accumulation'
            1: -0.5,  # e.g., 'Distribution/topping pattern'
            2: 0.0,   # e.g., 'Mean-reverting chop'
            3: 0.7,   # e.g., 'Confirmed uptrend continuation'
            4: -0.7,  # e.g., 'Confirmed downtrend continuation'
            -1: 0.0   # Default/noise cluster
        }
        # ----------------------------------------------------

        score = df[col].apply(lambda x: cluster_scores.get(x, 0.0))
        return score.fillna(0)

    def _normalize_sentiment_score(self, df):
        """
        Placeholder for incorporating sentiment data (e.g., from news/social).
        Assumes a 'sentiment_score' column exists (-1 bearish, +1 bullish).
        """
        col = 'sentiment_score' # Assume this column is added during data retrieval/prep
        if col not in df.columns:
             #print("Info: 'sentiment_score' not found. Returning neutral score (0).")
            return pd.Series(0, index=df.index, dtype=float)
        return df[col].fillna(0)

    def calculate_score(self, df_features):
        """
        Calculates the conviction score based on various integrated factors.

        Args:
            df_features (pd.DataFrame): DataFrame containing all necessary input features.
                                        Expected columns include those for signals, regime,
                                        ML outputs (patterns, clusters, anomalies), etc.

        Returns:
            pd.DataFrame: Input DataFrame with an added 'conviction_score' column.
        """
        if not isinstance(df_features, pd.DataFrame):
            raise ValueError("Input df_features must be a pandas DataFrame.")

        df_out = df_features.copy()
        print("Input columns for conviction score:", df_out.columns.tolist())

        # Calculate normalized scores for each component
        print("Calculating Adaptive Signals Score...")
        adaptive_signal_score = self._normalize_adaptive_signals(df_out)
        print("Calculating Market Regime Score...")
        market_regime_score = self._normalize_market_regime(df_out)
        print("Calculating ML Pattern Score...")
        ml_pattern_score = self._normalize_ml_pattern(df_out)
        print("Calculating Anomaly Score...")
        anomaly_contrib_score = self._normalize_anomaly_score(df_out) # Normalized anomaly measure
        print("Calculating Cluster Modifier Score...")
        cluster_modifier_score = self._normalize_cluster_modifier(df_out)
        print("Calculating Sentiment Score...")
        sentiment_score = self._normalize_sentiment_score(df_out) # Placeholder

        # --- Calculate Final Weighted Conviction Score ---
        print("Calculating final weighted score...")
        final_score = pd.Series(0.0, index=df_out.index) # Initialize score column

        # Add contributions safely, checking weights exist
        if 'adaptive_signals' in self.weights and self.weights['adaptive_signals'] != 0:
             final_score += self.weights['adaptive_signals'] * adaptive_signal_score
        if 'market_regime' in self.weights and self.weights['market_regime'] != 0:
             final_score += self.weights['market_regime'] * market_regime_score
        if 'ml_pattern' in self.weights and self.weights['ml_pattern'] != 0:
             final_score += self.weights['ml_pattern'] * ml_pattern_score
        if 'anomaly_score' in self.weights and self.weights['anomaly_score'] != 0:
             # Note: Anomaly score weight is expected to be negative
             final_score += self.weights['anomaly_score'] * anomaly_contrib_score
        if 'cluster_modifier' in self.weights and self.weights['cluster_modifier'] != 0:
             final_score += self.weights['cluster_modifier'] * cluster_modifier_score
        if 'sentiment_score' in self.weights and self.weights['sentiment_score'] != 0:
             final_score += self.weights['sentiment_score'] * sentiment_score # Placeholder

        # --- Post-processing ---
        # Ensure score is clipped between -1 and 1
        df_out['conviction_score'] = final_score.clip(-1, 1).fillna(0) # Fill NaNs again just in case

        print("Conviction score calculated and added to DataFrame.")
        # Optional: Print score stats or tail for debugging
        # print(df_out[['conviction_score', 'market_regime', 'ml_pattern_confidence', 'anomaly_score']].tail())
        # print(df_out['conviction_score'].describe())
        return df_out

# --- Example Usage Remains Mostly the Same for Testing ---
if __name__ == '__main__':
    # Create a dummy DataFrame similar to what df_features might look like after all calculations
    dates = pd.date_range(start='2023-01-01', periods=100, freq='D')
    data = {
        'close': np.random.rand(100) * 100 + 100,
        # Example adaptive signal triggers (already processed by Threshold module)
        'adaptive_buy_rsi': np.random.randint(0, 2, 100),
        'adaptive_sell_rsi': np.random.randint(0, 2, 100),
        'adaptive_buy_stoch_k': np.random.randint(0, 2, 100),
        'adaptive_sell_stoch_k': np.random.randint(0, 2, 100),
        # Calculated summary (alternative input style)
        'individual_buy_signals_adaptive': np.random.randint(0, 5, 100), # Could be pre-calculated
        'individual_sell_signals_adaptive': np.random.randint(0, 5, 100),# Could be pre-calculated
        # Regime detection output
        'market_regime': np.random.choice(['trending_up', 'trending_down', 'ranging', 'volatile'], 100),
        # Pattern Recognition output
        'ml_pattern_confidence': np.random.uniform(-1, 1, 100), # Ready score
        # Anomaly Detection output
        'anomaly_score': np.random.uniform(0, 5, 100), # Raw score, needs scaling
        # Clustering output
        'ml_cluster_cluster': np.random.choice([0, 1, 2, 3, -1], 100), # Cluster ID
        # Placeholder for sentiment
        'sentiment_score': np.random.uniform(-1, 1, 100), # Idealized score
    }
    dummy_df = pd.DataFrame(data, index=dates)

    # Initialize calculator (using default weights for demo)
    calculator = ConvictionScoreCalculator()

    # Calculate conviction score
    df_with_conviction = calculator.calculate_score(dummy_df)

    print("\nDataFrame with Conviction Score (last 5 rows):")
    print(df_with_conviction[['close', 'market_regime', 'ml_pattern_confidence', 'anomaly_score', 'ml_cluster_cluster','conviction_score']].tail())

    print("\nConviction Score statistics:")
    print(df_with_conviction['conviction_score'].describe())

    # Test with missing ML/Cluster/Anomaly columns to see default behavior
    print("\nTesting with missing optional columns (ML pattern, anomaly, cluster):")
    cols_to_drop = ['ml_pattern_confidence', 'anomaly_score', 'ml_cluster_cluster']
    dummy_df_missing = dummy_df.drop(columns=[col for col in cols_to_drop if col in dummy_df.columns])
    df_missing_conviction = calculator.calculate_score(dummy_df_missing)
    print(df_missing_conviction[['close', 'market_regime','conviction_score']].tail())
    print(df_missing_conviction['conviction_score'].describe())